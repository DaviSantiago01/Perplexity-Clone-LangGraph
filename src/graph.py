# Importa√ß√µes necess√°rias para o funcionamento do sistema
from pydantic import BaseModel

# LangChain e LangGraph - Framework para cria√ß√£o de agentes e workflows
from langchain_ollama import ChatOllama  # Interface para modelos Ollama
from langgraph.graph import START, END, StateGraph  # Componentes para criar grafos de estados
from langgraph.types import Send  # Tipo para envio de mensagens entre n√≥s
from pydantic.v1.typing import display_as_type
from tavily import TavilyClient  # Cliente para pesquisa web via API Tavily

# Importa√ß√£o dos m√≥dulos locais
import prompts  # Templates de prompts para os LLMs
from schemas import *  # Estruturas de dados (ReportState, QueryResult)
from prompts import *  # Prompts espec√≠ficos para cada etapa

# Carregamento de vari√°veis de ambiente (API keys)
from dotenv import load_dotenv
load_dotenv()

# Interface web com Streamlit
import streamlit as st

# Configura√ß√£o dos modelos LLM
# Modelo menor para tarefas simples (gera√ß√£o de queries)
llm = ChatOllama(model="llama3.2:1b")
# Modelo maior para racioc√≠nio complexo (resposta final)
llm_reasoning = ChatOllama(model="llama3.2:3b")

# ============================================================================
# N√ìDULOS DO LANGGRAPH - Cada fun√ß√£o representa um agente/etapa do workflow
# ============================================================================

def build_first_queries(state: ReportState):
    """
    PRIMEIRO N√ìDULO: Gerador de Queries de Pesquisa
    
    Esta fun√ß√£o √© o ponto de entrada do workflow. Ela recebe a pergunta do usu√°rio
    e gera m√∫ltiplas queries de pesquisa para obter informa√ß√µes abrangentes.
    
    Args:
        state (ReportState): Estado atual contendo a pergunta do usu√°rio
        
    Returns:
        dict: Dicion√°rio com lista de queries geradas
    """
    # Classe para for√ßar estrutura espec√≠fica na resposta do LLM
    class QueryList(BaseModel):
        queries: List[str] = []  # Lista obrigat√≥ria de strings (queries de pesquisa)

    # Extrai a pergunta original do estado compartilhado
    user_input = state.user_input

    # Formata o prompt substituindo a vari√°vel {user_input}
    prompt = build_queries.format(user_input=user_input)
    
    # Configura LLM para retornar APENAS no formato QueryList (structured output)
    query_llm = llm.with_structured_output(QueryList)
    
    # Executa o modelo e obt√©m as queries estruturadas
    result = query_llm.invoke(prompt)

    # Retorna as queries para serem usadas pelos pr√≥ximos n√≥dulos
    return {"queries": result.queries}

def spawn_researchers(state: ReportState):
    """
    FUN√á√ÉO DE ROTEAMENTO: Distribuidor de Pesquisas Paralelas
    
    Esta fun√ß√£o implementa o padr√£o "fan-out" do LangGraph, criando m√∫ltiplas
    execu√ß√µes paralelas do n√≥dulo 'single_search' - uma para cada query gerada.
    
    Args:
        state (ReportState): Estado contendo as queries a serem pesquisadas
        
    Returns:
        List[Send]: Lista de objetos Send para execu√ß√£o paralela
    """
    # Cria uma inst√¢ncia Send para cada query, permitindo execu√ß√£o paralela
    # Cada Send direciona para o n√≥dulo 'single_search' com uma query espec√≠fica
    return [Send("single_search", {"query": query}) for query in state.queries]

def single_search(state: dict):
    """
    SEGUNDO N√ìDULO: Pesquisador Individual (Execu√ß√£o Paralela)
    
    Esta fun√ß√£o executa uma pesquisa web individual usando a API Tavily,
    extrai o conte√∫do da p√°gina e gera um resumo focado na query espec√≠fica.
    M√∫ltiplas inst√¢ncias desta fun√ß√£o rodam em paralelo.
    
    Args:
        state (dict): Dicion√°rio contendo a query espec√≠fica para pesquisa
        
    Returns:
        dict: Resultado da pesquisa estruturado em QueryResult
    """
    # Extrai a query espec√≠fica enviada pelo spawn_researchers
    query = state["query"]
    
    # Inicializa cliente Tavily para pesquisa web
    tavily_client = TavilyClient()
    
    # Executa pesquisa web (m√°ximo 1 resultado por query)
    results = tavily_client.search(query, max_results=1, include_raw_content=False)

    # Obt√©m URL do primeiro resultado
    url = results["results"][0]["url"]
    
    # Extrai conte√∫do completo da p√°gina
    url_extraction = tavily_client.extract(url)

    # Verifica se a extra√ß√£o foi bem-sucedida
    if len(url_extraction["results"]) > 0:
        # Obt√©m conte√∫do bruto da p√°gina
        raw_content = url_extraction["results"][0]["raw_content"]
        
        # Formata prompt para resumir conte√∫do relevante √† query
        prompt = resume_search.format(user_input=query, search_results=raw_content)

        # Gera resumo usando LLM menor (mais r√°pido)
        llm_result = llm.invoke(prompt)

        # Estrutura resultado da pesquisa
        query_result = QueryResult(
            title=results["results"][0]["title"],
            url=url,
            resume=llm_result.content
        )
        
        # Retorna resultado para agrega√ß√£o no estado global
        return {"queries_results": [query_result]}

def final_write(state: ReportState):
    """
    TERCEIRO N√ìDULO: Compilador de Resposta Final
    
    Esta fun√ß√£o agrega todos os resultados das pesquisas paralelas e gera
    uma resposta final abrangente usando o modelo LLM mais potente.
    Implementa o padr√£o "fan-in" do LangGraph.
    
    Args:
        state (ReportState): Estado contendo todos os resultados das pesquisas
        
    Returns:
        dict: Resposta final formatada com refer√™ncias
    """
    # Inicializa strings para compilar resultados e refer√™ncias
    search_results = ""
    references = ""

    # Itera sobre todos os resultados das pesquisas paralelas
    for i, result in enumerate(state.queries_results):
        # Formata cada resultado de pesquisa de forma estruturada
        search_results += f"{i+1}\n\n"
        search_results += f"Title: {result.title}\n"
        search_results += f"URL: {result.url}\n"
        search_results += f"Resume: {result.resume}\n"
        search_results += f"===========\n\n"
        
        # Constr√≥i lista de refer√™ncias numeradas
        references += f"[{i+1}] {result.title} - {result.url}\n"

    # Formata prompt final com pergunta original e todos os resultados
    prompt = build_final_response.format(
        user_input=state.user_input, 
        search_results=search_results
    )
    
    # Usa modelo mais potente para racioc√≠nio complexo e s√≠ntese
    llm_result = llm_reasoning.invoke(prompt)
    
    # Combina resposta do LLM com refer√™ncias formatadas
    final_response = llm_result.content + "\n\n References: \n" + references

    # Retorna resposta final para o estado
    return {"final_response": final_response}


# ============================================================================
# CONFIGURA√á√ÉO DO LANGGRAPH - Defini√ß√£o do workflow e conex√µes
# ============================================================================

# Cria o construtor do grafo com o estado compartilhado ReportState
builder = StateGraph(ReportState)

# Adiciona os n√≥dulos (fun√ß√µes) ao grafo
builder.add_node("build_first_queries", build_first_queries)  # Gerador de queries
builder.add_node("single_search", single_search)              # Pesquisador paralelo
builder.add_node("final_write", final_write)                  # Compilador final

# Define as conex√µes (edges) entre os n√≥dulos
builder.add_edge(START, "build_first_queries")  # Inicia com gera√ß√£o de queries

# Conex√£o condicional: distribui queries para pesquisas paralelas
builder.add_conditional_edges(
    "build_first_queries",    # N√≥dulo de origem
    spawn_researchers,         # Fun√ß√£o de roteamento
    ["single_search"]         # N√≥dulos de destino
)

# Conecta pesquisas ao compilador final
builder.add_edge("single_search", "final_write")

# Finaliza o workflow
builder.add_edge("final_write", END)

# Compila o grafo em um objeto execut√°vel
graph = builder.compile()

# ============================================================================
# INTERFACE STREAMLIT - Aplica√ß√£o web para intera√ß√£o com o usu√°rio
# ============================================================================

if __name__ == "__main__":
    # Visualiza√ß√£o opcional do grafo (requer IPython)
    try:
        from IPython.display import Image, display
        display(Image(graph.get_graph().draw_mermaid_png()))
    except ImportError:
        pass  # IPython n√£o dispon√≠vel em ambiente Streamlit

    # Configura√ß√£o da interface web
    st.title("üîç Perplexity Clone - Ollama + LangGraph")
    st.markdown("*Pesquisa inteligente com IA local usando Ollama e busca web via Tavily*")
    
    # Campo de entrada para a pergunta do usu√°rio
    user_input = st.text_input(
        "Fa√ßa sua pergunta:", 
        placeholder="Ex: Quais s√£o as √∫ltimas novidades em IA?"
    )

    # Bot√£o para iniciar a pesquisa
    if st.button("üöÄ Pesquisar", type="primary"):
        if user_input.strip():  # Verifica se h√° input v√°lido
            # Interface de progresso durante execu√ß√£o
            with st.status("üîÑ Processando sua pergunta...", expanded=True) as status:
                st.write("üìù Gerando queries de pesquisa...")
                
                # Executa o workflow LangGraph em modo stream para feedback em tempo real
                for output in graph.stream(
                    {"user_input": user_input},
                    stream_mode="debug"  # Modo debug para acompanhar execu√ß√£o
                ):
                    # Mostra progresso de cada n√≥dulo executado
                    if output["type"] == "task_result":
                        node_name = output['payload']['name']
                        st.write(f"‚úÖ Executando: {node_name}")
                
                status.update(label="‚úÖ Pesquisa conclu√≠da!", state="complete")
            
            # Extrai e exibe a resposta final
            try:
                # Obt√©m resultado final do √∫ltimo output
                final_result = graph.invoke({"user_input": user_input})
                final_response = final_result.get("final_response", "Erro ao gerar resposta")
                
                # Exibe resposta formatada
                st.markdown("### üìã Resposta:")
                st.markdown(final_response)
                
            except Exception as e:
                st.error(f"‚ùå Erro durante processamento: {str(e)}")
        else:
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta v√°lida.")
 

