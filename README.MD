# Clone Perplexity - Motor de Busca Inteligente

> Sistema de busca que combina IA generativa com pesquisa web em tempo real

---

## üìã Sobre

Motor de busca que transforma perguntas em pesquisas paralelas, sintetizando informa√ß√µes em respostas completas com cita√ß√µes autom√°ticas.

**Principais recursos:**
- üîÑ Busca paralela em m√∫ltiplas fontes
- ‚ö° Duas LLMs: GPT-20B (queries + resumos) e GPT-120B (s√≠ntese final)
- üíæ Estado persistente (LangGraph v1.0)
- üìö Cita√ß√µes autom√°ticas

---

## ‚öôÔ∏è Instala√ß√£o

```bash
git clone https://github.com/DaviSantiago01/perplexity-clone.git
cd perplexity-clone

python -m venv venv 

# Linux/Mac:
source venv/bin/activate

# Windows (CMD):
venv\Scripts\activate.bat

# Windows (PowerShell):
venv\Scripts\Activate.ps1

pip install -r requirements.txt
```

**Configure suas APIs** (gratuitas): [Groq](https://console.groq.com/keys) ‚Ä¢ [Tavily](https://tavily.com/#api)

```bash
# Crie .env
GROQ_API_KEY=sua_chave
TAVILY_API_KEY=sua_chave
```

---

## üöÄ Uso

```bash
# CLI
python main.py

# Web UI
streamlit run app.py
```

---

## üìÅ Estrutura

```
perplexity-clone/
‚îú‚îÄ‚îÄ main.py              # CLI
‚îú‚îÄ‚îÄ app.py               # Streamlit UI
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ config.py        # Configura√ß√µes
    ‚îú‚îÄ‚îÄ models.py        # TypedDict
    ‚îú‚îÄ‚îÄ prompts.py       # Templates
    ‚îú‚îÄ‚îÄ graph.py         # LangGraph workflow
    ‚îî‚îÄ‚îÄ services.py      # Groq + Tavily
```

---

## üèóÔ∏è Como Funciona

```mermaid
flowchart TB
    A[üìù Pergunta] --> B[üîç Gera Queries]
    B --> |GPT-20B: 3-5 queries| C[üåê Busca Paralela]
    
    C --> C1[Tavily 1]
    C --> C2[Tavily 2]
    C --> C3[Tavily 3]
    
    C1 --> D[üìÑ Resumo]
    C2 --> D
    C3 --> D
    
    D --> |GPT-20B| E[üìä Consolida]
    E --> |GPT-120B| F[üß† S√≠ntese]
    F --> G[‚úÖ Resposta + Fontes]
```

**Fluxo:** Pergunta ‚Üí Queries (20B) ‚Üí Busca Paralela ‚Üí Resumos (20B) ‚Üí S√≠ntese (120B) ‚Üí Resposta

---

## üí° Exemplo

**Input:** `"Como fazer caf√© expresso?"`

**Processamento:**

```mermaid
flowchart LR 
    A["‚òï Como fazer caf√© expresso?"] --> B["üîç GPT-20B gera queries"]
    
    B --> C1["temperatura √°gua caf√©"]
    B --> C2["moagem gr√£os expresso"]
    B --> C3["tempo extra√ß√£o"]
    
    C1 --> D1["üåê Tavily busca 1"]
    C2 --> D2["üåê Tavily busca 2"]
    C3 --> D3["üåê Tavily busca 3"]
    
    D1 --> E1["üìÑ Resumo: 90-96¬∞C"]
    D2 --> E2["üìÑ Resumo: moagem fina"]
    D3 --> E3["üìÑ Resumo: 25-30 seg"]
    
    E1 --> F["üß† GPT-120B sintetiza tudo"]
    E2 --> F
    E3 --> F
    
    F --> G["‚úÖ Resposta completa + fontes"]
```

1. Sistema gera 3 queries otimizadas
2. Busca paralela em cada uma
3. Resume resultados individuais (GPT-20B)
4. Sintetiza resposta completa (GPT-120B)

**Output:**
```
Para caf√© expresso perfeito:
- Moagem fina e uniforme [1]
- √Ågua a 90-96¬∞C [2]
- Dose: 18-20g de caf√©
- Extra√ß√£o: 25-30 segundos [3]

Resultado: crema dourada e sabor equilibrado.

[1] barista-institute.com/guia-expresso
[2] coffeeresearch.org/temperature
[3] specialtycoffee.com/timing
```

---

---

## Cr√©ditos

Projeto inspirado no tutorial da [Asimov Academy](https://www.youtube.com/@AsimovAcademy). V√≠deo original: [Crie um Clone do Perplexity AI com LangGraph](https://www.youtube.com/watch?v=q2XPEjQ4Yt0). Esta vers√£o foi atualizada para LangGraph v1.0 e LangChain v1.0.