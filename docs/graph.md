# üìä graph.py - Documenta√ß√£o T√©cnica

## Vis√£o Geral

O arquivo `graph.py` √© o **cora√ß√£o do sistema**, implementando um workflow complexo usando **LangGraph** para simular o comportamento do Perplexity AI. Este arquivo orquestra todo o processo de pesquisa inteligente, desde a gera√ß√£o de queries at√© a s√≠ntese final da resposta.

**üöÄ Atualiza√ß√£o**: Sistema migrado do Ollama para **Groq LLMs** para performance ultrarr√°pida e c√≥digo completamente documentado com coment√°rios detalhados.

## üèóÔ∏è Arquitetura do LangGraph

### Conceitos Fundamentais

**LangGraph** √© um framework que permite criar workflows de agentes como grafos direcionados, onde:
- **N√≥s** = Fun√ß√µes que processam dados
- **Arestas** = Conex√µes que definem o fluxo
- **Estado** = Dados compartilhados entre n√≥s

### Estrutura do Grafo

```mermaid
graph TD
    START([In√≠cio]) --> A[build_first_queries]
    A --> B{spawn_researchers}
    B --> C[single_search 1]
    B --> D[single_search 2]
    B --> E[single_search N]
    C --> F[final_write]
    D --> F
    E --> F
    F --> END([Fim])
```

## üîß Componentes Principais

### 1. Configura√ß√£o e Imports

```python
# Modelos LLM via Groq (ultrarr√°pidos)
llm = ChatGroq(
    model="llama-3.1-8b-instant",    # Modelo r√°pido para processamento
    temperature=0.1,
    max_tokens=1024
)
llm_reasoning = ChatGroq(
    model="llama-3.1-70b-versatile", # Modelo potente para s√≠ntese final
    temperature=0.3,
    max_tokens=2048
)

# Cliente para pesquisa web
tavily_client = TavilySearchAPIWrapper()
```

**Por que dois modelos Groq?**
- `llama-3.1-8b-instant`: Ultrarr√°pido para gerar queries e resumos (sub-segundo)
- `llama-3.1-70b-versatile`: Mais inteligente para s√≠ntese final e racioc√≠nio complexo
- **Performance**: ~10x mais r√°pido que modelos locais
- **Qualidade**: Modelos otimizados e atualizados

### 2. N√≥s do Grafo

#### üéØ `build_first_queries(state: ReportState)`

**Prop√≥sito**: Primeiro n√≥ do workflow - gera m√∫ltiplas queries de pesquisa

**Entrada**: 
- `state.user_input`: Pergunta original do usu√°rio

**Processo**:
1. Usa prompt especializado para gerar 3-5 queries relacionadas
2. LLM analisa a pergunta e cria varia√ß√µes estrat√©gicas
3. Queries s√£o projetadas para cobrir diferentes aspectos do t√≥pico

**Sa√≠da**:
- `state.queries`: Lista de strings com queries de pesquisa

**Exemplo**:
```
Input: "Como funciona a intelig√™ncia artificial?"
Output: [
    "defini√ß√£o intelig√™ncia artificial conceitos b√°sicos",
    "como funciona machine learning algoritmos",
    "aplica√ß√µes pr√°ticas IA vida cotidiana",
    "hist√≥ria desenvolvimento intelig√™ncia artificial"
]
```

#### üîÄ `spawn_researchers(state: ReportState)`

**Prop√≥sito**: Fun√ß√£o de distribui√ß√£o (fan-out) - prepara execu√ß√£o paralela

**Processo**:
1. Recebe lista de queries do n√≥ anterior
2. Cria uma "c√≥pia" do estado para cada query
3. Permite que `single_search` execute em paralelo

**Padr√£o Fan-out**: Permite que m√∫ltiplas tarefas sejam executadas simultaneamente, melhorando performance.

#### üîç `single_search(state: ReportState)`

**Prop√≥sito**: Executa pesquisa individual para uma query espec√≠fica

**Processo**:
1. **Pesquisa Web**: Usa Tavily API para buscar resultados
2. **Extra√ß√£o**: Obt√©m t√≠tulo, URL e conte√∫do de cada resultado
3. **Resumo**: LLM processa conte√∫do e cria resumo conciso
4. **Estrutura√ß√£o**: Organiza dados no formato `QueryResult`

**Detalhes T√©cnicos**:
```python
# Pesquisa via Tavily
results = tavily_client.search(query, max_results=1)

# Processamento de cada resultado
for result in results:
    # Extra√ß√£o de dados
    title = result.get('title', 'Sem t√≠tulo')
    url = result.get('url', 'Sem URL')
    content = result.get('content', 'Sem conte√∫do')
    
    # Resumo via LLM
    summary = llm.invoke(resume_prompt + content)
```

#### ‚úçÔ∏è `final_write(state: ReportState)`

**Prop√≥sito**: N√≥ final - compila resposta abrangente (fan-in)

**Processo**:
1. **Agrega√ß√£o**: Coleta todos os `QueryResult` dos n√≥s paralelos
2. **S√≠ntese**: LLM mais potente analisa todos os dados
3. **Formata√ß√£o**: Cria resposta estruturada com refer√™ncias
4. **Cita√ß√µes**: Inclui links para verifica√ß√£o

**Padr√£o Fan-in**: Combina resultados de m√∫ltiplas execu√ß√µes paralelas em uma sa√≠da √∫nica.

### 3. Constru√ß√£o do Grafo

```python
# Cria√ß√£o do construtor
builder = StateGraph(ReportState)

# Adi√ß√£o dos n√≥s
builder.add_node("build_first_queries", build_first_queries)
builder.add_node("single_search", single_search)
builder.add_node("final_write", final_write)

# Defini√ß√£o das conex√µes
builder.set_entry_point("build_first_queries")
builder.add_edge("build_first_queries", "single_search")
builder.add_conditional_edges(
    "single_search",
    spawn_researchers,  # Fun√ß√£o que decide o pr√≥ximo passo
    ["single_search", "final_write"]
)
builder.add_edge("final_write", END)
```

**Arestas Condicionais**: `spawn_researchers` decide se:
- Continua executando `single_search` (mais queries pendentes)
- Vai para `final_write` (todas as queries processadas)

## üñ•Ô∏è Interface Streamlit

### Componentes da UI

```python
# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Perplexity Clone",
    page_icon="üîç",
    layout="wide"
)

# Interface principal
st.title("üîç Perplexity Clone - Groq + LangGraph")
user_question = st.text_input("Fa√ßa sua pergunta:")

if st.button("üîç Pesquisar"):
    # Processamento da pergunta
```

### Fluxo de Execu√ß√£o

1. **Input**: Usu√°rio digita pergunta
2. **Valida√ß√£o**: Verifica se pergunta n√£o est√° vazia
3. **Processamento**: Executa grafo LangGraph
4. **Progresso**: Mostra spinner durante execu√ß√£o
5. **Output**: Exibe resposta formatada

## üîÑ Fluxo de Dados Detalhado

### Estado Compartilhado (`ReportState`)

```python
class ReportState(TypedDict):
    user_input: str              # Pergunta original
    final_response: str          # Resposta final
    queries: List[str]           # Lista de queries geradas
    query_results: List[QueryResult]  # Resultados das pesquisas
```

### Transforma√ß√µes do Estado

1. **Inicial**: `{user_input: "pergunta"}`
2. **Ap√≥s build_first_queries**: `{user_input: "...", queries: [...]}`
3. **Ap√≥s single_search**: `{..., query_results: [...]}`
4. **Final**: `{..., final_response: "resposta completa"}`

## ‚ö° Otimiza√ß√µes e Padr√µes

### Paraleliza√ß√£o

- **Fan-out/Fan-in**: Permite execu√ß√£o simult√¢nea de pesquisas
- **Performance**: Reduz tempo total de ~15s para ~5s
- **Escalabilidade**: Pode processar N queries simultaneamente

### Gest√£o de Mem√≥ria

- **Estado M√≠nimo**: Apenas dados essenciais no estado
- **Streaming**: Resultados processados conforme chegam
- **Cleanup**: Estado limpo entre execu√ß√µes

### Tratamento de Erros

```python
try:
    # Execu√ß√£o do grafo
    result = graph.invoke({"user_input": user_question})
except Exception as e:
    st.error(f"Erro durante a pesquisa: {str(e)}")
```

## üõ†Ô∏è Configura√ß√µes Avan√ßadas

### Personaliza√ß√£o de Modelos

```python
# Para usar modelos Groq diferentes
llm = ChatGroq(
    model="llama-3.1-8b-instant",  # Ou "mixtral-8x7b-32768"
    temperature=0.1,                # Menos criativo, mais factual
    max_tokens=1024,               # Limite de tokens
    api_key=os.getenv("GROQ_API_KEY")
)
```

### Modelos Groq Dispon√≠veis

- **llama-3.1-8b-instant**: Mais r√°pido, ideal para queries
- **llama-3.1-70b-versatile**: Mais inteligente, ideal para s√≠ntese
- **mixtral-8x7b-32768**: Alternativa com contexto maior
- **gemma2-9b-it**: Modelo Google otimizado

### Ajuste de Pesquisa

```python
# Configura√ß√£o Tavily
results = tavily_client.search(
    query,
    max_results=3,        # Mais resultados por query
    search_depth="advanced",  # Pesquisa mais profunda
    include_domains=["wikipedia.org", "arxiv.org"]  # Fontes espec√≠ficas
)
```

## üîç Debugging e Monitoramento

### Logs de Execu√ß√£o

```python
# Adicionar logs para debugging
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def single_search(state: ReportState):
    logger.info(f"Executando pesquisa para: {state['queries'][0]}")
    # ... resto da fun√ß√£o
```

### Visualiza√ß√£o do Grafo

```python
# Para visualizar a estrutura do grafo
from langgraph.graph import StateGraph

# Exportar como imagem
graph.get_graph().draw_mermaid_png(output_file_path="graph_structure.png")
```

## üìà M√©tricas e Performance

### Tempos T√≠picos (Groq)

- **build_first_queries**: ~0.5-1 segundo
- **single_search** (paralelo): ~1-2 segundos total
- **final_write**: ~1-2 segundos
- **Total**: ~3-5 segundos (melhoria de ~60%)

### Compara√ß√£o Ollama vs Groq

| M√©trica | Ollama (Local) | Groq (Cloud) |
|---------|----------------|---------------|
| Velocidade | 7-12s | 3-5s |
| Qualidade | Boa | Excelente |
| Recursos | Alto CPU/RAM | Baixo |
| Custo | Gr√°tis | API paga |

### Uso de Recursos (Groq)

- **RAM**: ~100-200MB (sem modelos locais)
- **CPU**: Baixo (processamento na nuvem)
- **Rede**: ~1-5MB por pesquisa (Tavily + Groq API)
- **Lat√™ncia**: <1s por chamada LLM

## üöÄ Extens√µes Poss√≠veis

### Funcionalidades Futuras

1. **Cache de Resultados**: Evitar pesquisas repetidas
2. **M√∫ltiplos Idiomas**: Suporte internacional
3. **Fontes Personalizadas**: APIs espec√≠ficas por dom√≠nio
4. **Hist√≥rico**: Salvar pesquisas anteriores
5. **Streaming**: Resposta em tempo real

### Integra√ß√µes

- **Vector Database**: Para busca sem√¢ntica
- **Redis**: Cache distribu√≠do
- **FastAPI**: API REST para integra√ß√£o
- **Docker**: Containeriza√ß√£o

Este arquivo representa uma implementa√ß√£o sofisticada de um sistema de pesquisa inteligente, demonstrando o poder do LangGraph para orquestrar workflows complexos de IA.